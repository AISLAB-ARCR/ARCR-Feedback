{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b43411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchmetrics.classification import BinaryAUROC, BinaryAveragePrecision\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a431bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c42fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0fd065",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb8c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e170089",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_columns = columns[:25]\n",
    "static_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9247900",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_columns = columns[25:-16]\n",
    "print(seq_columns[:12])\n",
    "print(seq_columns[12:19])\n",
    "print(seq_columns[19:31])\n",
    "print(seq_columns[31:43])\n",
    "print(seq_columns[43:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dcbac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "goutallier_columns = columns[-16:]\n",
    "goutallier_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a663ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns) == len(static_columns) + len(seq_columns) + len(goutallier_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb95606",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = \"POD 6M retear\"\n",
    "output_columns = [\"6M ASES\", \"6M CSS\", \"6M KSS\", \"6M VAS(activity)\", \"6M VAS(resting)\"]\n",
    "input_columns = static_columns + [column for column in seq_columns if column not in output_columns] + goutallier_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23baa483",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[input_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([y_train, X_train[output_columns]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4260e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(split):\n",
    "  assert split in [\"train\", \"val\", \"test\"]\n",
    "\n",
    "  X_file_name = f\"X_{split}.csv\"\n",
    "  y_file_name = f\"y_{split}.csv\"\n",
    "\n",
    "  X = pd.read_csv(X_file_name)\n",
    "  y = pd.read_csv(y_file_name)\n",
    "\n",
    "  X_np = X[input_columns].to_numpy()\n",
    "  y_np = pd.concat([y, X[output_columns]], axis=1).to_numpy()\n",
    "\n",
    "  X_tensor = torch.tensor(X_np, dtype=torch.float32)\n",
    "  y_tensor = torch.tensor(y_np, dtype=torch.float32)\n",
    "\n",
    "  return TensorDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f36ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = get_dataset(\"train\")\n",
    "valset = get_dataset(\"val\")\n",
    "testset = get_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cc6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = len(input_columns)\n",
    "out_features = len([label_column]) + len(output_columns)\n",
    "in_features, out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c1332",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(L.LightningModule):\n",
    "  def __init__(self, in_features, out_features):\n",
    "    super().__init__()\n",
    "\n",
    "    dropout = 0.3\n",
    "    self.mlp = nn.Sequential(\n",
    "      nn.Linear(in_features, 512),\n",
    "      nn.LayerNorm(512),\n",
    "      nn.GELU(),\n",
    "      nn.Dropout(dropout),\n",
    "\n",
    "      nn.Linear(512, 1024),\n",
    "      nn.LayerNorm(1024),\n",
    "      nn.GELU(),\n",
    "      nn.Dropout(dropout),\n",
    "\n",
    "      nn.Linear(1024, 512),\n",
    "      nn.LayerNorm(512),\n",
    "      nn.GELU(),\n",
    "\n",
    "      nn.Linear(512, 256),\n",
    "      nn.LayerNorm(256),\n",
    "      nn.GELU(),\n",
    "\n",
    "      nn.Linear(256, out_features)\n",
    "    )\n",
    "\n",
    "    self.train_roc = BinaryAUROC()\n",
    "    self.val_roc = BinaryAUROC()\n",
    "    self.test_roc = BinaryAUROC()\n",
    "    self.test_ap = BinaryAveragePrecision()\n",
    "\n",
    "  def forward(self, xb):\n",
    "    outputs = self.mlp(xb)\n",
    "    logits = outputs[:, :1]\n",
    "    regs = outputs[:, 1:]\n",
    "    return logits, regs\n",
    "\n",
    "  def _shared_step(self, batch, metric=True):\n",
    "    xb, yb = batch\n",
    "    clf_targets = yb[:, :1]\n",
    "    reg_targets = yb[:, 1:]\n",
    "\n",
    "    logits, regs = self.forward(xb)\n",
    "    clf_loss = F.binary_cross_entropy_with_logits(logits, clf_targets)\n",
    "    reg_loss = F.smooth_l1_loss(regs, reg_targets)\n",
    "    loss = clf_loss + reg_loss\n",
    "\n",
    "    return {\n",
    "      \"loss\": loss,\n",
    "      \"clf_loss\": clf_loss,\n",
    "      \"reg_loss\": reg_loss,\n",
    "      \"clf_logits\": logits.detach(),\n",
    "      \"clf_targets\": clf_targets.detach(),\n",
    "    }\n",
    "  \n",
    "  def training_step(self, batch, batch_idx):\n",
    "    out = self._shared_step(batch)\n",
    "\n",
    "    self.log(\"train/loss\", out[\"loss\"], on_epoch=True, prog_bar=True)\n",
    "    self.log(\"train/clf_loss\", out[\"clf_loss\"])\n",
    "    self.log(\"train/reg_loss\", out[\"reg_loss\"])\n",
    "\n",
    "    probs = out[\"clf_logits\"].sigmoid().flatten()\n",
    "    targets = out[\"clf_targets\"].flatten().to(torch.int)\n",
    "    self.train_roc.update(probs, targets)\n",
    "\n",
    "    return out[\"loss\"]\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    out = self._shared_step(batch)\n",
    "\n",
    "    self.log(\"val/loss\", out[\"loss\"], prog_bar=True)\n",
    "    self.log(\"val/clf_loss\", out[\"clf_loss\"])\n",
    "    self.log(\"val/reg_loss\", out[\"reg_loss\"])\n",
    "\n",
    "    probs = out[\"clf_logits\"].sigmoid().flatten()\n",
    "    targets = out[\"clf_targets\"].flatten().to(torch.int)\n",
    "    self.val_roc.update(probs, targets)\n",
    "    \n",
    "    return out[\"loss\"]\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    out = self._shared_step(batch)\n",
    "\n",
    "    self.log(\"test/loss\", out[\"loss\"], prog_bar=True)\n",
    "    self.log(\"test/clf_loss\", out[\"clf_loss\"])\n",
    "    self.log(\"test/reg_loss\", out[\"reg_loss\"])\n",
    "\n",
    "    probs = out[\"clf_logits\"].sigmoid().flatten()\n",
    "    targets = out[\"clf_targets\"].flatten().to(torch.int)\n",
    "    self.test_roc.update(probs, targets)\n",
    "    self.test_ap.update(probs, targets)\n",
    "    \n",
    "    return out[\"loss\"]\n",
    "  \n",
    "  def on_train_epoch_end(self):\n",
    "    self.log(\"train/roc\", self.train_roc.compute())\n",
    "    self.train_roc.reset()\n",
    "\n",
    "  def on_validation_epoch_end(self):\n",
    "    self.log(\"val/roc\", self.val_roc.compute())\n",
    "    self.val_roc.reset()\n",
    "\n",
    "  def on_test_epoch_end(self):\n",
    "    self.log(\"test/roc\", self.test_roc.compute())\n",
    "    self.log(\"test/ap\", self.test_ap.compute())\n",
    "    self.test_roc.reset()\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    return optim.AdamW(self.parameters(), lr=5e-5, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3242e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_logs = []\n",
    "# batch_size = 128\n",
    "# num_experiments = 10\n",
    "# for i in range(num_experiments):\n",
    "#   mlp = MLP(in_features, out_features)\n",
    "#   trainloader = DataLoader(trainset, batch_size=batch_size)\n",
    "#   valloader = DataLoader(valset, batch_size=batch_size)\n",
    "\n",
    "#   trainer = L.Trainer(max_epochs=10)\n",
    "#   trainer.fit(mlp, trainloader, valloader)\n",
    "#   val_logs.append(trainer.test(mlp, valloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12108f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "val_logs = []\n",
    "\n",
    "batch_size = 128\n",
    "max_epochs = 80\n",
    "num_experiments = 5\n",
    "\n",
    "for i in range(num_experiments):\n",
    "  mlp = MLP(in_features, out_features)\n",
    "  trainloader = DataLoader(trainset, batch_size=batch_size)\n",
    "  valloader  = DataLoader(valset,  batch_size=batch_size)\n",
    "\n",
    "  trainer = L.Trainer(\n",
    "    max_epochs=80,\n",
    "    callbacks=[ModelCheckpoint(monitor='val/roc', mode='max', save_top_k=1)]\n",
    "  )\n",
    "  trainer.fit(mlp, trainloader, valloader)\n",
    "  models.append(mlp)\n",
    "  val_logs.append(trainer.test(mlp, valloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72215148",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_aps = np.array([val_log[0][\"test/ap\"] for val_log in val_logs])\n",
    "val_rocs = np.array([val_log[0][\"test/roc\"] for val_log in val_logs])\n",
    "pd.DataFrame({\"ROC AUC\": val_rocs, \"PR AUC\": val_aps}).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4fae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_best_model():\n",
    "best_roc_idx = val_rocs.argmax()\n",
    "best_mlp = models[best_roc_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f25d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_set_stat(dataset):\n",
    "  _, y = dataset[:]\n",
    "  negative, positive = torch.bincount(y[:, 0].to(torch.int)).tolist()\n",
    "  samples = len(dataset)\n",
    "\n",
    "  print(f\"tatal   : {samples}\")\n",
    "  print(f\"negative: {negative:3} ({negative/samples*100:5.2f}%)\")\n",
    "  print(f\"positive: {positive:3} ({positive/samples*100:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee2341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"trainset (SMOTE)\")\n",
    "show_set_stat(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"valset\")\n",
    "show_set_stat(valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4763ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def forward_loader(model, dataloader):\n",
    "  all_logits = []\n",
    "  all_regs = []\n",
    "  all_clf_targets = []\n",
    "  all_reg_targets = []\n",
    "  \n",
    "  model.eval()\n",
    "  for xb, yb in dataloader:\n",
    "    logits, regs = mlp(xb)\n",
    "    all_logits.append(logits)\n",
    "    all_regs.append(regs)\n",
    "    all_clf_targets.append(yb[:, :1])\n",
    "    all_reg_targets.append(yb[:, 1:])\n",
    "\n",
    "  logits = torch.cat(all_logits).flatten()\n",
    "  regs = torch.cat(all_regs)\n",
    "  clf_targets = torch.cat(all_clf_targets).to(torch.int).flatten()\n",
    "  reg_targets = torch.cat(all_reg_targets)\n",
    "\n",
    "  return logits, regs, clf_targets, reg_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, regs, clf_targets, reg_targets = forward_loader(best_mlp, valloader)\n",
    "probs = logits.sigmoid()\n",
    "\n",
    "print(f\"logits.shape:      {logits.shape}\")\n",
    "print(f\"probs.shape:       {probs.shape}\")\n",
    "print(f\"regs.shape:        {regs.shape}\")\n",
    "print()\n",
    "print(f\"clf_targets.shape: {clf_targets.shape}\")\n",
    "print(f\"reg_targets.shape: {reg_targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, thresholds = precision_recall_curve(clf_targets, probs)\n",
    "thresholds = np.append(thresholds, 1.0)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, precisions, label='Precision', marker='o', markersize=3)\n",
    "plt.plot(thresholds, recalls, label='Recall', marker='x', markersize=3)\n",
    "\n",
    "plt.title(\"Precision & Recall vs Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06428065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_distributions(\n",
    "  y_score, y_true, *,\n",
    "  bins=40,\n",
    "  title=None,\n",
    "  density=False,\n",
    "  th_lines=(0.5,),\n",
    "):\n",
    "  y_true = np.asarray(y_true).astype(int)\n",
    "  y_score = np.asarray(y_score)\n",
    "\n",
    "  x_main = y_score\n",
    "  x_label = \"Predicted probability\"\n",
    "\n",
    "  pos = x_main[y_true == 1]\n",
    "  neg = x_main[y_true == 0]\n",
    "\n",
    "  xmin = np.min(x_main)\n",
    "  xmax = np.max(x_main)\n",
    "  bins_edges = np.linspace(xmin, xmax, bins+1)\n",
    "\n",
    "  plt.figure(figsize=(9, 5.5))\n",
    "  plt.hist(neg, bins=bins_edges, alpha=0.55, density=density,\n",
    "           label=f\"Negative (n={len(neg)})\", edgecolor=\"white\", linewidth=0.5)\n",
    "  plt.hist(pos, bins=bins_edges, alpha=0.55, density=density,\n",
    "           label=f\"Positive (n={len(pos)})\", edgecolor=\"white\", linewidth=0.5)\n",
    "\n",
    "  if th_lines:\n",
    "    for th in th_lines:\n",
    "      plt.axvline(th, linestyle=\"--\", linewidth=1.5)\n",
    "\n",
    "  plt.xlabel(x_label)\n",
    "  plt.ylabel(\"Density\" if density else \"Count\")\n",
    "  plt.title(title or \"Score distributions by class\")\n",
    "  plt.legend(loc=\"best\")\n",
    "  plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590cb0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_thresholds = np.linspace(0, 1, 11)[1:-1].tolist() # [0.1, 0.2, ... , 0.9]\n",
    "\n",
    "def test_thresholds(y_score, y_true, thresholds=default_thresholds, verbose=True):\n",
    "  accuracies = []\n",
    "  precisions = []\n",
    "  recalls = []\n",
    "  f1s = []\n",
    "  for threshold in thresholds:\n",
    "    bin_acc = BinaryAccuracy(threshold)\n",
    "    bin_precison = BinaryPrecision(threshold)\n",
    "    bin_recall = BinaryRecall(threshold)\n",
    "    bin_f1 = BinaryF1Score(threshold)\n",
    "\n",
    "    bin_acc.update(y_score, y_true)\n",
    "    bin_precison.update(y_score, y_true)\n",
    "    bin_recall.update(y_score, y_true)\n",
    "    bin_f1.update(y_score, y_true)\n",
    "\n",
    "    accuracies.append(bin_acc.compute().item())\n",
    "    precisions.append(bin_precison.compute().item())\n",
    "    recalls.append(bin_recall.compute().item())\n",
    "    f1s.append(bin_f1.compute().item())\n",
    "\n",
    "  result = pd.DataFrame({\n",
    "    \"threshold\": thresholds,\n",
    "    \"accuracy\": accuracies,\n",
    "    \"precison\": precisions,\n",
    "    \"recall\": recalls,\n",
    "    \"f1\": f1s\n",
    "  }).set_index(\"threshold\")\n",
    "\n",
    "  if verbose:\n",
    "    print(result)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ccf794",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.5399]\n",
    "test_thresholds(probs, clf_targets, thresholds)\n",
    "plot_score_distributions(probs, clf_targets, bins=40, density=False, th_lines=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6d1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testset)\n",
    "show_set_stat(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd512ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(testset, batch_size=batch_size)\n",
    "test_logs = trainer.test(mlp, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logits, test_regs, test_clf_targets, test_reg_targets = forward_loader(mlp, testloader)\n",
    "test_probs = test_logits.sigmoid()\n",
    "test_thresholds(test_probs, test_clf_targets, thresholds)\n",
    "plot_score_distributions(test_probs, test_clf_targets, bins=40, density=True, th_lines=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e0404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae20aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_columns = seq_columns[:12] + goutallier_columns[:4] + goutallier_columns[8:12]\n",
    "pre_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c980f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_columns = [column for column in columns if column not in static_columns + pre_columns + output_columns]\n",
    "mean_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_table = pd.read_csv(\"X_train.csv\")\n",
    "mean_table[\"age_group\"] = mean_table[\"나이\"] // 10 * 10\n",
    "\n",
    "group_columns = [\"성별 (M:1,F:2)\", \"age_group\"]\n",
    "mean_table = mean_table.groupby(group_columns)[mean_columns].mean().reset_index()\n",
    "mean_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_with_mean_dataset(split):\n",
    "  assert split in [\"val\", \"test\"]\n",
    "  X = pd.read_csv(f\"X_{split}.csv\")\n",
    "  y = pd.read_csv(f\"y_{split}.csv\")\n",
    "\n",
    "  indices = pd.concat([X[\"성별 (M:1,F:2)\"], X[\"나이\"] // 10 * 10], axis=1)\n",
    "  indices.columns = group_columns\n",
    "  mean_values = indices.merge(mean_table, on=group_columns, how=\"left\")\n",
    "\n",
    "  X[mean_columns] = mean_values[mean_columns]\n",
    "  X_np = X[input_columns].to_numpy()\n",
    "  y_np = pd.concat([y, X[output_columns]], axis=1).to_numpy()\n",
    "\n",
    "  X_tensor = torch.tensor(X_np, dtype=torch.float32)\n",
    "  y_tensor = torch.tensor(y_np, dtype=torch.float32)\n",
    "\n",
    "  return TensorDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7049e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pre_with_mean_set = get_pre_with_mean_dataset(\"val\")\n",
    "val_pre_with_mean_loader = DataLoader(val_pre_with_mean_set, batch_size=batch_size)\n",
    "val_pre_with_mean_logs = trainer.test(mlp, val_pre_with_mean_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041bc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, regs, clf_targets, reg_targets = forward_loader(mlp, val_pre_with_mean_loader)\n",
    "probs = logits.sigmoid()\n",
    "test_thresholds(probs, clf_targets, thresholds)\n",
    "plot_score_distributions(probs, clf_targets, bins=40, density=True, th_lines=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea548a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87529f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(data, data_grad, epsilon):\n",
    "  sign_data_grad = data_grad.sign()\n",
    "  perturbed_data = data + epsilon*sign_data_grad\n",
    "  return perturbed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0709c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_static_columns = len(static_columns)\n",
    "num_goutallier_columns= len(goutallier_columns)\n",
    "num_static_columns, num_goutallier_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e41405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fgsm_target_start = num_static_columns+12\n",
    "# fgsm_target_end = -num_goutallier_columns\n",
    "# fgsm_target_columns = input_columns[fgsm_target_start:fgsm_target_end]\n",
    "# fgsm_target_columns, len(fgsm_target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d94292",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm_target_indices = []\n",
    "fgsm_target_columns = []\n",
    "fgsm_target_features = [\"ERabd\", \"ERside\", \"FF\", \"IR\", \"MMTgrade\", \"MMTsec\", \"add\"]\n",
    "\n",
    "for idx, column in enumerate(input_columns):\n",
    "  for feature in fgsm_target_features:\n",
    "    if feature in column and \"0M\" not in column:\n",
    "      break\n",
    "  else:\n",
    "    continue\n",
    "\n",
    "  fgsm_target_indices.append(idx)\n",
    "  fgsm_target_columns.append(column)\n",
    "\n",
    "[(idx, column) for idx, column in zip(fgsm_target_indices, fgsm_target_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f536bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 0: '6M ASES'          -> maximize\n",
    "# # 1: '6M CSS'           -> maximize\n",
    "# # 2: '6M KSS'           -> maximize\n",
    "# # 3: '6M VAS(activity)' -> minimize\n",
    "# # 4: '6M VAS(resting)'  -> minimize\n",
    "# maximize_indices = [0, 1, 2]\n",
    "# minimize_indices = [3, 4]\n",
    "\n",
    "# lambda_logits = 1.0\n",
    "# lambda_reg = 0.3\n",
    "# epsilon = 0.1\n",
    "# num_fgsm = 10\n",
    "\n",
    "# all_logits = []\n",
    "# all_regs = []\n",
    "# all_perturbed_xb = []\n",
    "# all_perturbed_logits = []\n",
    "# all_perturbed_regs = []\n",
    "\n",
    "# mlp.eval()\n",
    "# for xb, yb in val_pre_with_mean_loader:\n",
    "#   clf_targets = yb[:, :1]\n",
    "#   reg_targets = yb[:, 1:]\n",
    "\n",
    "#   xb.requires_grad = True\n",
    "#   xb_target = xb[:, fgsm_target_indices]\n",
    "#   logits, regs = mlp(xb)\n",
    "#   all_logits.append(logits.detach())\n",
    "#   all_regs.append(regs.detach())\n",
    "\n",
    "#   clf_loss = F.binary_cross_entropy_with_logits(logits, clf_targets)\n",
    "#   logits_dir_loss = -logits.mean()\n",
    "\n",
    "#   reg_inc_term = -regs[:, maximize_indices].mean()\n",
    "#   reg_dec_term = regs[:, minimize_indices].mean()\n",
    "#   reg_dir_loss = reg_inc_term + reg_dec_term\n",
    "\n",
    "#   loss = clf_loss + lambda_logits * logits_dir_loss + lambda_reg * reg_dir_loss\n",
    "\n",
    "#   mlp.zero_grad()\n",
    "#   loss.backward()\n",
    "\n",
    "#   xb_target_grad = xb.grad.data[:, fgsm_target_indices]\n",
    "#   perturbed_xb_target = fgsm_attack(xb_target, xb_target_grad, epsilon)\n",
    "\n",
    "#   perturbed_xb = xb.detach().clone()\n",
    "#   perturbed_xb[:, fgsm_target_indices] = perturbed_xb_target\n",
    "#   all_perturbed_xb.append(perturbed_xb.detach())\n",
    "\n",
    "#   perturbed_logits, perturbed_regs = mlp(perturbed_xb)\n",
    "#   all_perturbed_logits.append(perturbed_logits.detach())\n",
    "#   all_perturbed_regs.append(perturbed_regs.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c2ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 0: '6M ASES'          -> maximize\n",
    "# # 1: '6M CSS'           -> maximize\n",
    "# # 2: '6M KSS'           -> maximize\n",
    "# # 3: '6M VAS(activity)' -> minimize\n",
    "# # 4: '6M VAS(resting)'  -> minimize\n",
    "maximize_indices = [0, 1, 2]\n",
    "minimize_indices = [3, 4]\n",
    "\n",
    "lambda_logits = 1.0\n",
    "lambda_reg = 0.3\n",
    "epsilon = 1e-2\n",
    "num_fgsm_attack = 50\n",
    "\n",
    "all_logits = []\n",
    "all_regs = []\n",
    "all_perturbed_xb = []\n",
    "all_perturbed_logits = []\n",
    "all_perturbed_regs = []\n",
    "\n",
    "mlp.eval()\n",
    "for xb, yb in val_pre_with_mean_loader:\n",
    "  clf_targets = yb[:, :1]\n",
    "  reg_targets = yb[:, 1:]\n",
    "\n",
    "  current_xb = xb.detach().clone()\n",
    "  current_xb.requires_grad = True\n",
    "\n",
    "  logits, regs = mlp(current_xb)\n",
    "  all_logits.append(logits.detach())\n",
    "  all_regs.append(regs.detach())\n",
    "\n",
    "  for _ in range(num_fgsm_attack):\n",
    "    clf_loss = F.binary_cross_entropy_with_logits(logits, clf_targets)\n",
    "\n",
    "    logits_dir_loss = -logits.mean()\n",
    "\n",
    "    reg_inc_term = regs[:, maximize_indices].mean()\n",
    "    reg_dec_term = -regs[:, minimize_indices].mean()\n",
    "    reg_dir_loss = reg_inc_term + reg_dec_term\n",
    "\n",
    "    loss = clf_loss + lambda_logits * logits_dir_loss + lambda_reg * reg_dir_loss\n",
    "\n",
    "    mlp.zero_grad()\n",
    "    if current_xb.grad is not None:\n",
    "      current_xb.grad.zero_()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    xb_target_grad = current_xb.grad.data[:, fgsm_target_indices]\n",
    "    xb_target = current_xb.detach()[:, fgsm_target_indices]\n",
    "\n",
    "    perturbed_xb_target = fgsm_attack(xb_target, xb_target_grad, epsilon)\n",
    "\n",
    "    updated_xb = current_xb.detach().clone()\n",
    "    updated_xb[:, fgsm_target_indices] = perturbed_xb_target\n",
    "\n",
    "    current_xb = updated_xb.detach().clone()\n",
    "    current_xb.requires_grad = True\n",
    "    logits, regs = mlp(current_xb)\n",
    "\n",
    "  all_perturbed_xb.append(current_xb.detach())\n",
    "\n",
    "  final_logits = logits\n",
    "  final_regs = regs\n",
    "\n",
    "  all_perturbed_logits.append(final_logits.detach())\n",
    "  all_perturbed_regs.append(final_regs.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b761ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.cat(all_logits).flatten()\n",
    "regs = torch.cat(all_regs)\n",
    "logits.shape, regs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce007506",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_logits = torch.cat(all_perturbed_logits).flatten()\n",
    "perturbed_regs = torch.cat(all_perturbed_regs)\n",
    "perturbed_logits.shape, perturbed_regs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a136de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = logits.sigmoid()\n",
    "perturbed_probs = perturbed_logits.sigmoid()\n",
    "probs.shape, perturbed_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe51f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_column)\n",
    "prob_results = pd.DataFrame({\n",
    "  \"probability\": probs.flatten(),\n",
    "  \"after probability\": perturbed_probs.flatten()\n",
    "})\n",
    "prob_results[\"delta\"] = prob_results[\"after probability\"] - prob_results[\"probability\"]\n",
    "prob_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e27a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_results = []\n",
    "for feature_idx in range(regs.size(1)):\n",
    "  feature_column = output_columns[feature_idx]\n",
    "  after_column = f\"after {feature_column}\"\n",
    "\n",
    "  feature_results = pd.DataFrame({\n",
    "    feature_column: regs[:, feature_idx],\n",
    "    after_column: perturbed_regs[:, feature_idx]\n",
    "  })\n",
    "  feature_results[\"delta\"] = feature_results[after_column] - feature_results[feature_column]\n",
    "  all_feature_results.append(feature_results)\n",
    "\n",
    "  direction = \"maximize\" if feature_idx in maximize_indices else \"minimize\"\n",
    "  print(f\"{feature_column} ({direction})\")\n",
    "  print(feature_results)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d6218",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_idx, (feature_column, feature_results) in enumerate(zip(output_columns, all_feature_results)):\n",
    "  direction = \"maximize\" if feature_idx in maximize_indices else \"minimize\"\n",
    "  print(f\"{feature_column} ({direction})\")\n",
    "  print(feature_results.describe())\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fccd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_xbs = torch.cat(all_perturbed_xb)[:, fgsm_target_indices]\n",
    "perturbed_xbs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6741828",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xb = []\n",
    "for xb, yb in val_pre_with_mean_loader:\n",
    "  all_xb.append(xb[:, fgsm_target_indices])\n",
    "xbs = torch.cat(all_xb)\n",
    "xbs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c7305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbs = xbs.cpu().detach().numpy()\n",
    "perturbed_xbs = perturbed_xbs.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = perturbed_xbs - xbs  # [batch, features]\n",
    "\n",
    "df_before = pd.DataFrame(xbs, columns=fgsm_target_columns)\n",
    "df_after = pd.DataFrame(perturbed_xbs, columns=fgsm_target_columns)\n",
    "df_delta = pd.DataFrame(delta, columns=fgsm_target_columns)\n",
    "\n",
    "mean_before = df_before.mean(axis=0)\n",
    "mean_after = df_after.mean(axis=0)\n",
    "mean_delta = df_delta.mean(axis=0)\n",
    "\n",
    "x = np.arange(len(fgsm_target_columns))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width/2, mean_before.values, width, label='before')\n",
    "plt.bar(x + width/2, mean_after.values, width, label='after')\n",
    "plt.xticks(x, fgsm_target_columns, rotation=45, ha='right')\n",
    "plt.ylabel(\"Mean value across batch\")\n",
    "plt.title(\"Feature-wise mean before vs after FGSM\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(x, mean_delta.values)\n",
    "plt.xticks(x, fgsm_target_columns, rotation=45, ha='right')\n",
    "plt.axhline(0, linewidth=1)\n",
    "plt.ylabel(\"Mean Δ (after - before)\")\n",
    "plt.title(\"Feature-wise mean change after FGSM\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "  \"mean_before\": mean_before,\n",
    "  \"mean_after\": mean_after,\n",
    "  \"mean_delta\": mean_delta\n",
    "})\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc967d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "scaler: StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeffe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_columns = list(scaler.get_feature_names_out())\n",
    "scaler_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a72a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scaled_samples = perturbed_xbs.shape[0]\n",
    "n_scaled_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e70227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_scale(scaler, xb):\n",
    "  n_scaled_samples = xb.shape[0]\n",
    "  scaled_data = pd.DataFrame(np.zeros((n_scaled_samples, len(scaler_columns))), columns=scaler_columns)\n",
    "  scaled_data[fgsm_target_columns] = xb\n",
    "  inversed_data = scaler.inverse_transform(scaled_data)\n",
    "  inversed_data = pd.DataFrame(inversed_data, columns=scaler_columns)\n",
    "  return inversed_data[fgsm_target_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8837afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_examples = inverse_scale(scaler, perturbed_xbs)\n",
    "perturbed_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_examples = inverse_scale(scaler, xbs)\n",
    "original_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b7e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_examples.to_csv(\"perturbed_examples.csv\", index=False)\n",
    "original_examples.to_csv(\"original_examples.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c867fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape, regs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef3576",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_probs.shape, perturbed_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_regs(scaler, regs):\n",
    "  n_scaled_samples = regs.shape[0]\n",
    "  scaled_data = pd.DataFrame(np.zeros((n_scaled_samples, len(scaler_columns))), columns=scaler_columns)\n",
    "  scaled_data[output_columns] = regs\n",
    "  inversed_data = scaler.inverse_transform(scaled_data)\n",
    "  inversed_data = pd.DataFrame(inversed_data, columns=scaler_columns)\n",
    "  return inversed_data[output_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59748fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_regs_df = inverse_regs(scaler, regs)\n",
    "original_regs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45cfa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_regs_df = inverse_regs(scaler, perturbed_regs)\n",
    "perturbed_regs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f40ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_probs_df = pd.DataFrame(probs, columns=[label_column])\n",
    "original_probs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_probs_df = pd.DataFrame(perturbed_probs, columns=[label_column])\n",
    "perturbed_probs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_outputs = pd.concat([original_probs_df, original_regs_df], axis=1)\n",
    "original_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_outputs = pd.concat([perturbed_probs_df, perturbed_regs_df], axis=1)\n",
    "perturbed_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_outputs.to_csv(\"original_outputs.csv\", index=False)\n",
    "perturbed_outputs.to_csv(\"perturbed_outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31545f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_val = val_pre_with_mean_set[:]\n",
    "pos_indicies = (y_val[:, :1] == 1.).flatten().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb3f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_examples[pos_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_examples[pos_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e139973",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_outputs[pos_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939aae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_outputs[pos_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(fgsm_target_columns))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width/2, mean_before.values, width, label='before')\n",
    "plt.bar(x + width/2, mean_after.values, width, label='after')\n",
    "plt.xticks(x, fgsm_target_columns, rotation=45, ha='right')\n",
    "plt.ylabel(\"Mean value across batch\")\n",
    "plt.title(\"Feature-wise mean before vs after FGSM\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56beb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_columns = fgsm_target_columns + [label_column] + output_columns \n",
    "result_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a9fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b303df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_examples.loc[[idx], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e552c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_outputs.loc[[idx], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7744bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = pd.concat([\n",
    "  original_examples.loc[[idx], :],\n",
    "  original_outputs.loc[[idx], :]\n",
    "], axis=1)\n",
    "before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a50668",
   "metadata": {},
   "outputs": [],
   "source": [
    "after = pd.concat([\n",
    "  perturbed_examples.loc[[idx], :],\n",
    "  perturbed_outputs.loc[[idx], :]\n",
    "], axis=1)\n",
    "after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08067db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([before, after], axis=0).to_csv(\"before_after.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acab7a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(result_columns[:-6]))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width/2, before.values.ravel()[:-6], width, label='before')\n",
    "plt.bar(x + width/2, after.values.ravel()[:-6], width, label='after')\n",
    "plt.xticks(x, result_columns[:-6], rotation=45, ha='right')\n",
    "plt.title(f\"Before vs After FGSM (index={idx})\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb473039",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(result_columns[-6:]))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width/2, before.values.ravel()[-6:], width, label='before')\n",
    "plt.bar(x + width/2, after.values.ravel()[-6:], width, label='after')\n",
    "plt.xticks(x, result_columns[-6:], rotation=45, ha='right')\n",
    "plt.title(f\"Before vs After FGSM (index={idx})\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(result_columns[-6:-5]))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width/2, before.values.ravel()[-6:-5], width, label='before')\n",
    "plt.bar(x + width/2, after.values.ravel()[-6:-5], width, label='after')\n",
    "plt.xticks(x, result_columns[-6:-5], rotation=45, ha='right')\n",
    "plt.title(f\"Before vs After FGSM (index={idx})\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(result_columns[-5:]))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width/2, before.values.ravel()[-5:], width, label='before')\n",
    "plt.bar(x + width/2, after.values.ravel()[-5:], width, label='after')\n",
    "plt.xticks(x, result_columns[-5:], rotation=45, ha='right')\n",
    "plt.title(f\"Before vs After FGSM (index={idx})\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06072a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

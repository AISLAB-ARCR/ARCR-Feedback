{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b43411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchmetrics.classification import BinaryAUROC, BinaryAveragePrecision\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a431bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c42fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0fd065",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb8c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e170089",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_columns = columns[:25]\n",
    "static_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9247900",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_columns = columns[25:-16]\n",
    "print(seq_columns[:12])\n",
    "print(seq_columns[12:19])\n",
    "print(seq_columns[19:31])\n",
    "print(seq_columns[31:43])\n",
    "print(seq_columns[43:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dcbac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "goutallier_columns = columns[-16:]\n",
    "goutallier_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a663ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns) == len(static_columns) + len(seq_columns) + len(goutallier_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb95606",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = \"POD 6M retear\"\n",
    "output_columns = [\"6M ASES\", \"6M CSS\", \"6M KSS\", \"6M VAS(activity)\", \"6M VAS(resting)\"]\n",
    "input_columns = static_columns + [column for column in seq_columns if column not in output_columns] + goutallier_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23baa483",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[input_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([y_train, X_train[output_columns]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4260e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(split):\n",
    "  assert split in [\"train\", \"val\", \"test\"]\n",
    "\n",
    "  X_file_name = f\"X_{split}.csv\"\n",
    "  y_file_name = f\"y_{split}.csv\"\n",
    "\n",
    "  X = pd.read_csv(X_file_name)\n",
    "  y = pd.read_csv(y_file_name)\n",
    "\n",
    "  X_np = X[input_columns].to_numpy()\n",
    "  y_np = pd.concat([y, X[output_columns]], axis=1).to_numpy()\n",
    "\n",
    "  X_tensor = torch.tensor(X_np, dtype=torch.float32)\n",
    "  y_tensor = torch.tensor(y_np, dtype=torch.float32)\n",
    "\n",
    "  return TensorDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f36ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = get_dataset(\"train\")\n",
    "valset = get_dataset(\"val\")\n",
    "testset = get_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cc6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = len(input_columns)\n",
    "out_features = len([label_column]) + len(output_columns)\n",
    "in_features, out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c1332",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(L.LightningModule):\n",
    "  def __init__(self, in_features, out_features):\n",
    "    super().__init__()\n",
    "\n",
    "    dropout = 0.3\n",
    "    self.mlp = nn.Sequential(\n",
    "      nn.Linear(in_features, 512),\n",
    "      nn.LayerNorm(512),\n",
    "      nn.GELU(),\n",
    "      nn.Dropout(dropout),\n",
    "\n",
    "      nn.Linear(512, 1024),\n",
    "      nn.LayerNorm(1024),\n",
    "      nn.GELU(),\n",
    "      nn.Dropout(dropout),\n",
    "\n",
    "      nn.Linear(1024, 512),\n",
    "      nn.LayerNorm(512),\n",
    "      nn.GELU(),\n",
    "\n",
    "      nn.Linear(512, 256),\n",
    "      nn.LayerNorm(256),\n",
    "      nn.GELU(),\n",
    "\n",
    "      nn.Linear(256, out_features)\n",
    "    )\n",
    "\n",
    "    self.train_roc = BinaryAUROC()\n",
    "    self.val_roc = BinaryAUROC()\n",
    "    self.test_roc = BinaryAUROC()\n",
    "    self.test_ap = BinaryAveragePrecision()\n",
    "\n",
    "  def forward(self, xb):\n",
    "    outputs = self.mlp(xb)\n",
    "    logits = outputs[:, :1]\n",
    "    regs = outputs[:, 1:]\n",
    "    return logits, regs\n",
    "\n",
    "  def _shared_step(self, batch, metric=True):\n",
    "    xb, yb = batch\n",
    "    clf_targets = yb[:, :1]\n",
    "    reg_targets = yb[:, 1:]\n",
    "\n",
    "    logits, regs = self.forward(xb)\n",
    "    clf_loss = F.binary_cross_entropy_with_logits(logits, clf_targets)\n",
    "    reg_loss = F.smooth_l1_loss(regs, reg_targets)\n",
    "    loss = clf_loss + reg_loss\n",
    "\n",
    "    return {\n",
    "      \"loss\": loss,\n",
    "      \"clf_loss\": clf_loss,\n",
    "      \"reg_loss\": reg_loss,\n",
    "      \"clf_logits\": logits.detach(),\n",
    "      \"clf_targets\": clf_targets.detach(),\n",
    "    }\n",
    "  \n",
    "  def training_step(self, batch, batch_idx):\n",
    "    out = self._shared_step(batch)\n",
    "\n",
    "    self.log(\"train/loss\", out[\"loss\"], on_epoch=True, prog_bar=True)\n",
    "    self.log(\"train/clf_loss\", out[\"clf_loss\"])\n",
    "    self.log(\"train/reg_loss\", out[\"reg_loss\"])\n",
    "\n",
    "    probs = out[\"clf_logits\"].sigmoid().flatten()\n",
    "    targets = out[\"clf_targets\"].flatten().to(torch.int)\n",
    "    self.train_roc.update(probs, targets)\n",
    "\n",
    "    return out[\"loss\"]\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    out = self._shared_step(batch)\n",
    "\n",
    "    self.log(\"val/loss\", out[\"loss\"], prog_bar=True)\n",
    "    self.log(\"val/clf_loss\", out[\"clf_loss\"])\n",
    "    self.log(\"val/reg_loss\", out[\"reg_loss\"])\n",
    "\n",
    "    probs = out[\"clf_logits\"].sigmoid().flatten()\n",
    "    targets = out[\"clf_targets\"].flatten().to(torch.int)\n",
    "    self.val_roc.update(probs, targets)\n",
    "    \n",
    "    return out[\"loss\"]\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    out = self._shared_step(batch)\n",
    "\n",
    "    self.log(\"test/loss\", out[\"loss\"], prog_bar=True)\n",
    "    self.log(\"test/clf_loss\", out[\"clf_loss\"])\n",
    "    self.log(\"test/reg_loss\", out[\"reg_loss\"])\n",
    "\n",
    "    probs = out[\"clf_logits\"].sigmoid().flatten()\n",
    "    targets = out[\"clf_targets\"].flatten().to(torch.int)\n",
    "    self.test_roc.update(probs, targets)\n",
    "    self.test_ap.update(probs, targets)\n",
    "    \n",
    "    return out[\"loss\"]\n",
    "  \n",
    "  def on_train_epoch_end(self):\n",
    "    self.log(\"train/roc\", self.train_roc.compute())\n",
    "    self.train_roc.reset()\n",
    "\n",
    "  def on_validation_epoch_end(self):\n",
    "    self.log(\"val/roc\", self.val_roc.compute())\n",
    "    self.val_roc.reset()\n",
    "\n",
    "  def on_test_epoch_end(self):\n",
    "    self.log(\"test/roc\", self.test_roc.compute())\n",
    "    self.log(\"test/ap\", self.test_ap.compute())\n",
    "    self.test_roc.reset()\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    return optim.AdamW(self.parameters(), lr=5e-5, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3242e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_logs = []\n",
    "# batch_size = 128\n",
    "# num_experiments = 10\n",
    "# for i in range(num_experiments):\n",
    "#   mlp = MLP(in_features, out_features)\n",
    "#   trainloader = DataLoader(trainset, batch_size=batch_size)\n",
    "#   valloader = DataLoader(valset, batch_size=batch_size)\n",
    "\n",
    "#   trainer = L.Trainer(max_epochs=10)\n",
    "#   trainer.fit(mlp, trainloader, valloader)\n",
    "#   val_logs.append(trainer.test(mlp, valloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12108f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "val_logs = []\n",
    "\n",
    "batch_size = 128\n",
    "max_epochs = 80\n",
    "num_experiments = 5\n",
    "\n",
    "for i in range(num_experiments):\n",
    "  mlp = MLP(in_features, out_features)\n",
    "  trainloader = DataLoader(trainset, batch_size=batch_size)\n",
    "  valloader  = DataLoader(valset,  batch_size=batch_size)\n",
    "\n",
    "  trainer = L.Trainer(\n",
    "    max_epochs=80,\n",
    "    callbacks=[ModelCheckpoint(monitor='val/roc', mode='max', save_top_k=1)]\n",
    "  )\n",
    "  trainer.fit(mlp, trainloader, valloader)\n",
    "  models.append(mlp)\n",
    "  val_logs.append(trainer.test(mlp, valloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72215148",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_aps = np.array([val_log[0][\"test/ap\"] for val_log in val_logs])\n",
    "val_rocs = np.array([val_log[0][\"test/roc\"] for val_log in val_logs])\n",
    "pd.DataFrame({\"ROC AUC\": val_rocs, \"PR AUC\": val_aps}).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4fae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_best_model():\n",
    "best_roc_idx = val_rocs.argmax()\n",
    "best_mlp = models[best_roc_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f25d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_set_stat(dataset):\n",
    "  _, y = dataset[:]\n",
    "  negative, positive = torch.bincount(y[:, 0].to(torch.int)).tolist()\n",
    "  samples = len(dataset)\n",
    "\n",
    "  print(f\"tatal   : {samples}\")\n",
    "  print(f\"negative: {negative:3} ({negative/samples*100:5.2f}%)\")\n",
    "  print(f\"positive: {positive:3} ({positive/samples*100:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee2341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"trainset (SMOTE)\")\n",
    "show_set_stat(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"valset\")\n",
    "show_set_stat(valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4763ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def forward_loader(model, dataloader):\n",
    "  all_logits = []\n",
    "  all_regs = []\n",
    "  all_clf_targets = []\n",
    "  all_reg_targets = []\n",
    "  \n",
    "  model.eval()\n",
    "  for xb, yb in dataloader:\n",
    "    logits, regs = mlp(xb)\n",
    "    all_logits.append(logits)\n",
    "    all_regs.append(regs)\n",
    "    all_clf_targets.append(yb[:, :1])\n",
    "    all_reg_targets.append(yb[:, 1:])\n",
    "\n",
    "  logits = torch.cat(all_logits).flatten()\n",
    "  regs = torch.cat(all_regs)\n",
    "  clf_targets = torch.cat(all_clf_targets).to(torch.int).flatten()\n",
    "  reg_targets = torch.cat(all_reg_targets)\n",
    "\n",
    "  return logits, regs, clf_targets, reg_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, regs, clf_targets, reg_targets = forward_loader(best_mlp, valloader)\n",
    "probs = logits.sigmoid()\n",
    "\n",
    "print(f\"logits.shape:      {logits.shape}\")\n",
    "print(f\"probs.shape:       {probs.shape}\")\n",
    "print(f\"regs.shape:        {regs.shape}\")\n",
    "print()\n",
    "print(f\"clf_targets.shape: {clf_targets.shape}\")\n",
    "print(f\"reg_targets.shape: {reg_targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, thresholds = precision_recall_curve(clf_targets, probs)\n",
    "thresholds = np.append(thresholds, 1.0)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, precisions, label='Precision', marker='o', markersize=3)\n",
    "plt.plot(thresholds, recalls, label='Recall', marker='x', markersize=3)\n",
    "\n",
    "plt.title(\"Precision & Recall vs Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06428065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_distributions(\n",
    "  y_score, y_true, *,\n",
    "  bins=40,\n",
    "  title=None,\n",
    "  density=False,\n",
    "  th_lines=(0.5,),\n",
    "):\n",
    "  y_true = np.asarray(y_true).astype(int)\n",
    "  y_score = np.asarray(y_score)\n",
    "\n",
    "  x_main = y_score\n",
    "  x_label = \"Predicted probability\"\n",
    "\n",
    "  pos = x_main[y_true == 1]\n",
    "  neg = x_main[y_true == 0]\n",
    "\n",
    "  xmin = np.min(x_main)\n",
    "  xmax = np.max(x_main)\n",
    "  bins_edges = np.linspace(xmin, xmax, bins+1)\n",
    "\n",
    "  plt.figure(figsize=(9, 5.5))\n",
    "  plt.hist(neg, bins=bins_edges, alpha=0.55, density=density,\n",
    "           label=f\"Negative (n={len(neg)})\", edgecolor=\"white\", linewidth=0.5)\n",
    "  plt.hist(pos, bins=bins_edges, alpha=0.55, density=density,\n",
    "           label=f\"Positive (n={len(pos)})\", edgecolor=\"white\", linewidth=0.5)\n",
    "\n",
    "  if th_lines:\n",
    "    for th in th_lines:\n",
    "      plt.axvline(th, linestyle=\"--\", linewidth=1.5)\n",
    "\n",
    "  plt.xlabel(x_label)\n",
    "  plt.ylabel(\"Density\" if density else \"Count\")\n",
    "  plt.title(title or \"Score distributions by class\")\n",
    "  plt.legend(loc=\"best\")\n",
    "  plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590cb0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_thresholds = np.linspace(0, 1, 11)[1:-1].tolist() # [0.1, 0.2, ... , 0.9]\n",
    "\n",
    "def test_thresholds(y_score, y_true, thresholds=default_thresholds, verbose=True):\n",
    "  accuracies = []\n",
    "  precisions = []\n",
    "  recalls = []\n",
    "  f1s = []\n",
    "  for threshold in thresholds:\n",
    "    bin_acc = BinaryAccuracy(threshold)\n",
    "    bin_precison = BinaryPrecision(threshold)\n",
    "    bin_recall = BinaryRecall(threshold)\n",
    "    bin_f1 = BinaryF1Score(threshold)\n",
    "\n",
    "    bin_acc.update(y_score, y_true)\n",
    "    bin_precison.update(y_score, y_true)\n",
    "    bin_recall.update(y_score, y_true)\n",
    "    bin_f1.update(y_score, y_true)\n",
    "\n",
    "    accuracies.append(bin_acc.compute().item())\n",
    "    precisions.append(bin_precison.compute().item())\n",
    "    recalls.append(bin_recall.compute().item())\n",
    "    f1s.append(bin_f1.compute().item())\n",
    "\n",
    "  result = pd.DataFrame({\n",
    "    \"threshold\": thresholds,\n",
    "    \"accuracy\": accuracies,\n",
    "    \"precison\": precisions,\n",
    "    \"recall\": recalls,\n",
    "    \"f1\": f1s\n",
    "  }).set_index(\"threshold\")\n",
    "\n",
    "  if verbose:\n",
    "    print(result)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ccf794",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.5399]\n",
    "test_thresholds(probs, clf_targets, thresholds)\n",
    "plot_score_distributions(probs, clf_targets, bins=40, density=False, th_lines=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6d1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testset)\n",
    "show_set_stat(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd512ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(testset, batch_size=batch_size)\n",
    "test_logs = trainer.test(mlp, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logits, test_regs, test_clf_targets, test_reg_targets = forward_loader(mlp, testloader)\n",
    "test_probs = test_logits.sigmoid()\n",
    "test_thresholds(test_probs, test_clf_targets, thresholds)\n",
    "plot_score_distributions(test_probs, test_clf_targets, bins=40, density=True, th_lines=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e0404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae20aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_columns = seq_columns[:12] + goutallier_columns[:4] + goutallier_columns[8:12]\n",
    "pre_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c980f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_columns = [column for column in columns if column not in static_columns + pre_columns + output_columns]\n",
    "mean_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_table = pd.read_csv(\"X_train.csv\")\n",
    "mean_table[\"age_group\"] = mean_table[\"나이\"] // 10 * 10\n",
    "\n",
    "group_columns = [\"성별 (M:1,F:2)\", \"age_group\"]\n",
    "mean_table = mean_table.groupby(group_columns)[mean_columns].mean().reset_index()\n",
    "mean_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_with_mean_dataset(split):\n",
    "  assert split in [\"val\", \"test\"]\n",
    "  X = pd.read_csv(f\"X_{split}.csv\")\n",
    "  y = pd.read_csv(f\"y_{split}.csv\")\n",
    "\n",
    "  indices = pd.concat([X[\"성별 (M:1,F:2)\"], X[\"나이\"] // 10 * 10], axis=1)\n",
    "  indices.columns = group_columns\n",
    "  mean_values = indices.merge(mean_table, on=group_columns, how=\"left\")\n",
    "\n",
    "  X[mean_columns] = mean_values[mean_columns]\n",
    "  X_np = X[input_columns].to_numpy()\n",
    "  y_np = pd.concat([y, X[output_columns]], axis=1).to_numpy()\n",
    "\n",
    "  X_tensor = torch.tensor(X_np, dtype=torch.float32)\n",
    "  y_tensor = torch.tensor(y_np, dtype=torch.float32)\n",
    "\n",
    "  return TensorDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7049e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pre_with_mean_set = get_pre_with_mean_dataset(\"val\")\n",
    "val_pre_with_mean_loader = DataLoader(val_pre_with_mean_set, batch_size=batch_size)\n",
    "val_pre_with_mean_logs = trainer.test(mlp, val_pre_with_mean_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041bc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, regs, clf_targets, reg_targets = forward_loader(mlp, val_pre_with_mean_loader)\n",
    "probs = logits.sigmoid()\n",
    "test_thresholds(probs, clf_targets, thresholds)\n",
    "plot_score_distributions(probs, clf_targets, bins=40, density=True, th_lines=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea548a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87529f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(data, data_grad, epsilon):\n",
    "  sign_data_grad = data_grad.sign()\n",
    "  perturbed_data = data + epsilon*sign_data_grad\n",
    "  return perturbed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0709c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_static_columns = len(static_columns)\n",
    "num_goutallier_columns= len(goutallier_columns)\n",
    "num_static_columns, num_goutallier_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e41405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fgsm_target_start = num_static_columns+12\n",
    "# fgsm_target_end = -num_goutallier_columns\n",
    "# fgsm_target_columns = input_columns[fgsm_target_start:fgsm_target_end]\n",
    "# fgsm_target_columns, len(fgsm_target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d94292",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm_target_indices = []\n",
    "fgsm_target_columns = []\n",
    "fgsm_target_features = [\"ERabd\", \"ERside\", \"FF\", \"IR\", \"MMTgrade\", \"MMTsec\", \"add\"]\n",
    "\n",
    "for idx, column in enumerate(input_columns):\n",
    "  for feature in fgsm_target_features:\n",
    "    if feature in column and \"0M\" not in column:\n",
    "      break\n",
    "  else:\n",
    "    continue\n",
    "\n",
    "  fgsm_target_indices.append(idx)\n",
    "  fgsm_target_columns.append(column)\n",
    "\n",
    "[(idx, column) for idx, column in zip(fgsm_target_indices, fgsm_target_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f536bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 0: '6M ASES'          -> maximize\n",
    "# # 1: '6M CSS'           -> maximize\n",
    "# # 2: '6M KSS'           -> maximize\n",
    "# # 3: '6M VAS(activity)' -> minimize\n",
    "# # 4: '6M VAS(resting)'  -> minimize\n",
    "# maximize_indices = [0, 1, 2]\n",
    "# minimize_indices = [3, 4]\n",
    "\n",
    "# lambda_logits = 1.0\n",
    "# lambda_reg = 0.3\n",
    "# epsilon = 0.1\n",
    "# num_fgsm = 10\n",
    "\n",
    "# all_logits = []\n",
    "# all_regs = []\n",
    "# all_perturbed_xb = []\n",
    "# all_perturbed_logits = []\n",
    "# all_perturbed_regs = []\n",
    "\n",
    "# mlp.eval()\n",
    "# for xb, yb in val_pre_with_mean_loader:\n",
    "#   clf_targets = yb[:, :1]\n",
    "#   reg_targets = yb[:, 1:]\n",
    "\n",
    "#   xb.requires_grad = True\n",
    "#   xb_target = xb[:, fgsm_target_indices]\n",
    "#   logits, regs = mlp(xb)\n",
    "#   all_logits.append(logits.detach())\n",
    "#   all_regs.append(regs.detach())\n",
    "\n",
    "#   clf_loss = F.binary_cross_entropy_with_logits(logits, clf_targets)\n",
    "#   logits_dir_loss = -logits.mean()\n",
    "\n",
    "#   reg_inc_term = -regs[:, maximize_indices].mean()\n",
    "#   reg_dec_term = regs[:, minimize_indices].mean()\n",
    "#   reg_dir_loss = reg_inc_term + reg_dec_term\n",
    "\n",
    "#   loss = clf_loss + lambda_logits * logits_dir_loss + lambda_reg * reg_dir_loss\n",
    "\n",
    "#   mlp.zero_grad()\n",
    "#   loss.backward()\n",
    "\n",
    "#   xb_target_grad = xb.grad.data[:, fgsm_target_indices]\n",
    "#   perturbed_xb_target = fgsm_attack(xb_target, xb_target_grad, epsilon)\n",
    "\n",
    "#   perturbed_xb = xb.detach().clone()\n",
    "#   perturbed_xb[:, fgsm_target_indices] = perturbed_xb_target\n",
    "#   all_perturbed_xb.append(perturbed_xb.detach())\n",
    "\n",
    "#   perturbed_logits, perturbed_regs = mlp(perturbed_xb)\n",
    "#   all_perturbed_logits.append(perturbed_logits.detach())\n",
    "#   all_perturbed_regs.append(perturbed_regs.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c2ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 0: '6M ASES'          -> maximize\n",
    "# # 1: '6M CSS'           -> maximize\n",
    "# # 2: '6M KSS'           -> maximize\n",
    "# # 3: '6M VAS(activity)' -> minimize\n",
    "# # 4: '6M VAS(resting)'  -> minimize\n",
    "maximize_indices = [0, 1, 2]\n",
    "minimize_indices = [3, 4]\n",
    "\n",
    "lambda_logits = 1.0\n",
    "lambda_reg = 0.3\n",
    "epsilon = 1e-2\n",
    "num_fgsm_attack = 50\n",
    "\n",
    "all_logits = []\n",
    "all_regs = []\n",
    "all_perturbed_xb = []\n",
    "all_perturbed_logits = []\n",
    "all_perturbed_regs = []\n",
    "\n",
    "mlp.eval()\n",
    "for xb, yb in val_pre_with_mean_loader:\n",
    "  clf_targets = yb[:, :1]\n",
    "  reg_targets = yb[:, 1:]\n",
    "\n",
    "  current_xb = xb.detach().clone()\n",
    "  current_xb.requires_grad = True\n",
    "\n",
    "  logits, regs = mlp(current_xb)\n",
    "  all_logits.append(logits.detach())\n",
    "  all_regs.append(regs.detach())\n",
    "\n",
    "  for _ in range(num_fgsm_attack):\n",
    "    clf_loss = F.binary_cross_entropy_with_logits(logits, clf_targets)\n",
    "\n",
    "    logits_dir_loss = -logits.mean()\n",
    "\n",
    "    reg_inc_term = regs[:, maximize_indices].mean()\n",
    "    reg_dec_term = -regs[:, minimize_indices].mean()\n",
    "    reg_dir_loss = reg_inc_term + reg_dec_term\n",
    "\n",
    "    loss = clf_loss + lambda_logits * logits_dir_loss + lambda_reg * reg_dir_loss\n",
    "\n",
    "    mlp.zero_grad()\n",
    "    if current_xb.grad is not None:\n",
    "      current_xb.grad.zero_()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    xb_target_grad = current_xb.grad.data[:, fgsm_target_indices]\n",
    "    xb_target = current_xb.detach()[:, fgsm_target_indices]\n",
    "\n",
    "    perturbed_xb_target = fgsm_attack(xb_target, xb_target_grad, epsilon)\n",
    "\n",
    "    updated_xb = current_xb.detach().clone()\n",
    "    updated_xb[:, fgsm_target_indices] = perturbed_xb_target\n",
    "\n",
    "    current_xb = updated_xb.detach().clone()\n",
    "    current_xb.requires_grad = True\n",
    "    logits, regs = mlp(current_xb)\n",
    "\n",
    "  all_perturbed_xb.append(current_xb.detach())\n",
    "\n",
    "  final_logits = logits\n",
    "  final_regs = regs\n",
    "\n",
    "  all_perturbed_logits.append(final_logits.detach())\n",
    "  all_perturbed_regs.append(final_regs.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b761ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.cat(all_logits).flatten()\n",
    "regs = torch.cat(all_regs)\n",
    "logits.shape, regs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce007506",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_logits = torch.cat(all_perturbed_logits).flatten()\n",
    "perturbed_regs = torch.cat(all_perturbed_regs)\n",
    "perturbed_logits.shape, perturbed_regs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a136de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = logits.sigmoid()\n",
    "perturbed_probs = perturbed_logits.sigmoid()\n",
    "probs.shape, perturbed_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe51f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_column)\n",
    "prob_results = pd.DataFrame({\n",
    "  \"probability\": probs.flatten(),\n",
    "  \"after probability\": perturbed_probs.flatten()\n",
    "})\n",
    "prob_results[\"delta\"] = prob_results[\"after probability\"] - prob_results[\"probability\"]\n",
    "prob_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e27a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_results = []\n",
    "for feature_idx in range(regs.size(1)):\n",
    "  feature_column = output_columns[feature_idx]\n",
    "  after_column = f\"after {feature_column}\"\n",
    "\n",
    "  feature_results = pd.DataFrame({\n",
    "    feature_column: regs[:, feature_idx],\n",
    "    after_column: perturbed_regs[:, feature_idx]\n",
    "  })\n",
    "  feature_results[\"delta\"] = feature_results[after_column] - feature_results[feature_column]\n",
    "  all_feature_results.append(feature_results)\n",
    "\n",
    "  direction = \"maximize\" if feature_idx in maximize_indices else \"minimize\"\n",
    "  print(f\"{feature_column} ({direction})\")\n",
    "  print(feature_results)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d6218",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_idx, (feature_column, feature_results) in enumerate(zip(output_columns, all_feature_results)):\n",
    "  direction = \"maximize\" if feature_idx in maximize_indices else \"minimize\"\n",
    "  print(f\"{feature_column} ({direction})\")\n",
    "  print(feature_results.describe())\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fccd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_xbs = torch.cat(all_perturbed_xb)[:, fgsm_target_indices]\n",
    "perturbed_xbs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6741828",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xb = []\n",
    "for xb, yb in val_pre_with_mean_loader:\n",
    "  all_xb.append(xb[:, fgsm_target_indices])\n",
    "xbs = torch.cat(all_xb)\n",
    "xbs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c7305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbs = xbs.cpu().detach().numpy()\n",
    "perturbed_xbs = perturbed_xbs.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = perturbed_xbs - xbs  # [batch, features]\n",
    "\n",
    "df_before = pd.DataFrame(xbs, columns=fgsm_target_columns)\n",
    "df_after = pd.DataFrame(perturbed_xbs, columns=fgsm_target_columns)\n",
    "df_delta = pd.DataFrame(delta, columns=fgsm_target_columns)\n",
    "\n",
    "mean_before = df_before.mean(axis=0)\n",
    "mean_after = df_after.mean(axis=0)\n",
    "mean_delta = df_delta.mean(axis=0)\n",
    "\n",
    "x = np.arange(len(fgsm_target_columns))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width/2, mean_before.values, width, label='before')\n",
    "plt.bar(x + width/2, mean_after.values, width, label='after')\n",
    "plt.xticks(x, fgsm_target_columns, rotation=45, ha='right')\n",
    "plt.ylabel(\"Mean value across batch\")\n",
    "plt.title(\"Feature-wise mean before vs after FGSM\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(x, mean_delta.values)\n",
    "plt.xticks(x, fgsm_target_columns, rotation=45, ha='right')\n",
    "plt.axhline(0, linewidth=1)\n",
    "plt.ylabel(\"Mean Δ (after - before)\")\n",
    "plt.title(\"Feature-wise mean change after FGSM\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "  \"mean_before\": mean_before,\n",
    "  \"mean_after\": mean_after,\n",
    "  \"mean_delta\": mean_delta\n",
    "})\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc967d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "scaler: StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeffe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_columns = list(scaler.get_feature_names_out())\n",
    "scaler_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a72a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scaled_samples = perturbed_xbs.shape[0]\n",
    "n_scaled_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e70227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_scale(scaler, xb):\n",
    "  n_scaled_samples = xb.shape[0]\n",
    "  scaled_data = pd.DataFrame(np.zeros((n_scaled_samples, len(scaler_columns))), columns=scaler_columns)\n",
    "  scaled_data[fgsm_target_columns] = xb\n",
    "  inversed_data = scaler.inverse_transform(scaled_data)\n",
    "  inversed_data = pd.DataFrame(inversed_data, columns=scaler_columns)\n",
    "  return inversed_data[fgsm_target_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8837afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_examples = inverse_scale(scaler, perturbed_xbs)\n",
    "perturbed_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_examples = inverse_scale(scaler, xbs)\n",
    "original_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b7e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_examples.to_csv(\"perturbed_examples.csv\", index=False)\n",
    "original_examples.to_csv(\"original_examples.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c867fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape, regs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef3576",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_probs.shape, perturbed_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_regs(scaler, regs):\n",
    "  n_scaled_samples = regs.shape[0]\n",
    "  scaled_data = pd.DataFrame(np.zeros((n_scaled_samples, len(scaler_columns))), columns=scaler_columns)\n",
    "  scaled_data[output_columns] = regs\n",
    "  inversed_data = scaler.inverse_transform(scaled_data)\n",
    "  inversed_data = pd.DataFrame(inversed_data, columns=scaler_columns)\n",
    "  return inversed_data[output_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59748fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_regs_df = inverse_regs(scaler, regs)\n",
    "original_regs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45cfa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_regs_df = inverse_regs(scaler, perturbed_regs)\n",
    "perturbed_regs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f40ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_probs_df = pd.DataFrame(probs, columns=[label_column])\n",
    "original_probs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_probs_df = pd.DataFrame(perturbed_probs, columns=[label_column])\n",
    "perturbed_probs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_outputs = pd.concat([original_probs_df, original_regs_df], axis=1)\n",
    "original_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_outputs = pd.concat([perturbed_probs_df, perturbed_regs_df], axis=1)\n",
    "perturbed_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_outputs.to_csv(\"original_outputs.csv\", index=False)\n",
    "perturbed_outputs.to_csv(\"perturbed_outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31545f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_val = val_pre_with_mean_set[:]\n",
    "pos_indicies = (y_val[:, :1] == 1.).flatten().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb3f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_examples[pos_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_examples[pos_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e139973",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_outputs[pos_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939aae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_outputs[pos_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(fgsm_target_columns))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width/2, mean_before.values, width, label='before')\n",
    "plt.bar(x + width/2, mean_after.values, width, label='after')\n",
    "plt.xticks(x, fgsm_target_columns, rotation=45, ha='right')\n",
    "plt.ylabel(\"Mean value across batch\")\n",
    "plt.title(\"Feature-wise mean before vs after FGSM\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56beb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_columns = fgsm_target_columns + [label_column] + output_columns \n",
    "result_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a9fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b303df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_examples.loc[[idx], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e552c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_outputs.loc[[idx], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7744bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = pd.concat([\n",
    "  original_examples.loc[[idx], :],\n",
    "  original_outputs.loc[[idx], :]\n",
    "], axis=1)\n",
    "before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a50668",
   "metadata": {},
   "outputs": [],
   "source": [
    "after = pd.concat([\n",
    "  perturbed_examples.loc[[idx], :],\n",
    "  perturbed_outputs.loc[[idx], :]\n",
    "], axis=1)\n",
    "after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08067db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([before, after], axis=0).to_csv(\"before_after.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acab7a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(result_columns[:-6]))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width/2, before.values.ravel()[:-6], width, label='before')\n",
    "plt.bar(x + width/2, after.values.ravel()[:-6], width, label='after')\n",
    "plt.xticks(x, result_columns[:-6], rotation=45, ha='right')\n",
    "plt.title(f\"Before vs After FGSM (index={idx})\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb473039",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(result_columns[-6:]))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width/2, before.values.ravel()[-6:], width, label='before')\n",
    "plt.bar(x + width/2, after.values.ravel()[-6:], width, label='after')\n",
    "plt.xticks(x, result_columns[-6:], rotation=45, ha='right')\n",
    "plt.title(f\"Before vs After FGSM (index={idx})\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(result_columns[-6:-5]))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width/2, before.values.ravel()[-6:-5], width, label='before')\n",
    "plt.bar(x + width/2, after.values.ravel()[-6:-5], width, label='after')\n",
    "plt.xticks(x, result_columns[-6:-5], rotation=45, ha='right')\n",
    "plt.title(f\"Before vs After FGSM (index={idx})\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(result_columns[-5:]))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width/2, before.values.ravel()[-5:], width, label='before')\n",
    "plt.bar(x + width/2, after.values.ravel()[-5:], width, label='after')\n",
    "plt.xticks(x, result_columns[-5:], rotation=45, ha='right')\n",
    "plt.title(f\"Before vs After FGSM (index={idx})\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06072a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f3852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a37c869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cab6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def forward_loader(model, dataloader):\n",
    "  all_logits, all_regs, all_clf_targets, all_reg_targets = [], [], [], []\n",
    "  model.eval()\n",
    "  for xb, yb in dataloader:\n",
    "    logits, regs = model(xb)\n",
    "    all_logits.append(logits)\n",
    "    all_regs.append(regs)\n",
    "    all_clf_targets.append(yb[:, :1])\n",
    "    all_reg_targets.append(yb[:, 1:])\n",
    "  logits = torch.cat(all_logits).flatten()\n",
    "  regs = torch.cat(all_regs)\n",
    "  clf_targets = torch.cat(all_clf_targets).to(torch.int).flatten()\n",
    "  reg_targets = torch.cat(all_reg_targets)\n",
    "  return logits, regs, clf_targets, reg_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "mono_groups = {}\n",
    "pattern = re.compile(r\"(\\d+)M\\s+(.+)\")\n",
    "for idx, col in enumerate(input_columns):\n",
    "  m = pattern.match(col)\n",
    "  if m is None: \n",
    "    continue\n",
    "  month = int(m.group(1))\n",
    "  feat  = m.group(2).strip()\n",
    "  if idx not in fgsm_target_indices:\n",
    "    continue\n",
    "  mono_groups.setdefault(feat, []).append((month, idx))\n",
    "\n",
    "for feat in mono_groups:\n",
    "    mono_groups[feat] = sorted(mono_groups[feat], key=lambda x: x[0])\n",
    "\n",
    "increasing_features = {\"ERabd\", \"ERside\", \"FF\", \"IR\", \"MMTgrade\", \"MMTsec\", \"add\"}\n",
    "decreasing_features = {\"VAS\"}\n",
    "\n",
    "def feature_direction(feat: str) -> str:\n",
    "  base = feat\n",
    "  if any(k in base for k in increasing_features):\n",
    "    return \"inc\"\n",
    "  if any(k in base for k in decreasing_features):\n",
    "    return \"dec\"\n",
    "  return \"inc\"\n",
    "\n",
    "def monotonic_and_smoothness_penalty(xb, margin=0.0):\n",
    "  mono_terms = []\n",
    "  smooth_terms = []\n",
    "\n",
    "  for feat, seq in mono_groups.items():\n",
    "    if len(seq) < 2:\n",
    "      continue\n",
    "    dirn = feature_direction(feat)\n",
    "    idxs = [col_idx for (_, col_idx) in seq]\n",
    "    xs = xb[:, idxs] # [B, T]\n",
    "\n",
    "    diffs = xs[:, 1:] - xs[:, :-1] # [B, T-1]\n",
    "    if dirn == \"inc\":\n",
    "      mono = F.relu(-diffs + margin).mean()\n",
    "    else:\n",
    "      mono = F.relu(diffs + margin).mean()\n",
    "    mono_terms.append(mono)\n",
    "\n",
    "    if xs.size(1) >= 3:\n",
    "      x_tminus1 = xs[:, :-2]\n",
    "      x_t = xs[:, 1:-1]\n",
    "      x_tplus1 = xs[:, 2:]\n",
    "      second_diff = (x_tplus1 - 2*x_t + x_tminus1) # [B, T-2]\n",
    "      smooth = (second_diff ** 2).mean()\n",
    "      smooth_terms.append(smooth)\n",
    "\n",
    "  mono_loss = torch.stack(mono_terms).mean() if mono_terms else xb.new_tensor(0.0)\n",
    "  smooth_loss = torch.stack(smooth_terms).mean() if smooth_terms else xb.new_tensor(0.0)\n",
    "  return mono_loss, smooth_loss\n",
    "\n",
    "epsilon_total = 0.08    \n",
    "step_size = 0.004   \n",
    "pgd_steps = 50\n",
    "\n",
    "lambda_logits = 1.0 \n",
    "lambda_regdir = 0.3 \n",
    "lambda_mono = 0.5 \n",
    "lambda_smooth = 0.1 \n",
    "mono_margin = 0.00\n",
    "\n",
    "std_clip_min = -4.0\n",
    "std_clip_max =  4.0\n",
    "\n",
    "mlp.eval()\n",
    "all_pgd_xb = []\n",
    "all_pgd_logits = []\n",
    "all_pgd_regs = []\n",
    "\n",
    "for xb, yb in val_pre_with_mean_loader:\n",
    "  clf_targets = yb[:, :1]\n",
    "  reg_targets = yb[:, 1:]\n",
    "\n",
    "  original_xb = xb.detach().clone()\n",
    "  current_xb = xb.detach().clone().requires_grad_(True)\n",
    "\n",
    "  logits, regs = mlp(current_xb)\n",
    "\n",
    "  for _ in range(pgd_steps):\n",
    "    clf_loss = F.binary_cross_entropy_with_logits(logits, clf_targets)\n",
    "    logits_dir_loss = -logits.mean()\n",
    "    reg_inc_term = +regs[:, maximize_indices].mean() \n",
    "    reg_dec_term = -regs[:, minimize_indices].mean() \n",
    "    reg_dir_loss = reg_inc_term + reg_dec_term\n",
    "\n",
    "    mono_loss, smooth_loss = monotonic_and_smoothness_penalty(current_xb, margin=mono_margin)\n",
    "    loss = clf_loss + lambda_logits*logits_dir_loss + lambda_regdir*reg_dir_loss + lambda_mono*mono_loss + lambda_smooth*smooth_loss\n",
    "\n",
    "    mlp.zero_grad()\n",
    "    if current_xb.grad is not None:\n",
    "      current_xb.grad.zero_()\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      grad = current_xb.grad\n",
    "      delta = torch.zeros_like(current_xb)\n",
    "      delta[:, fgsm_target_indices] = step_size * grad[:, fgsm_target_indices].sign()\n",
    "\n",
    "      updated = current_xb + delta\n",
    "\n",
    "      diff = torch.clamp(updated - original_xb, min=-epsilon_total, max=epsilon_total)\n",
    "      projected = original_xb + diff\n",
    "      projected = torch.clamp(projected, std_clip_min, std_clip_max)\n",
    "      current_xb = projected.detach().clone().requires_grad_(True)\n",
    "      logits, regs = mlp(current_xb)\n",
    "\n",
    "  all_pgd_xb.append(current_xb.detach())\n",
    "  all_pgd_logits.append(logits.detach())\n",
    "  all_pgd_regs.append(regs.detach())\n",
    "\n",
    "pgd_perturbed_xb  = torch.cat(all_pgd_xb)\n",
    "pgd_perturbed_log = torch.cat(all_pgd_logits).flatten()\n",
    "pgd_perturbed_reg = torch.cat(all_pgd_regs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_mean_trajectories(xb_dict):\n",
    "  for feat, seq in mono_groups.items():\n",
    "    months = [m for (m, _) in seq]\n",
    "    idxs   = [idx for (_, idx) in seq]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    for label, arr in xb_dict.items():\n",
    "      xs = arr[:, idxs].cpu().numpy()  # [N, T]\n",
    "      mean = xs.mean(axis=0)\n",
    "      std  = xs.std(axis=0)\n",
    "      plt.plot(months, mean, marker='o', label=label)\n",
    "      plt.fill_between(months, mean-std, mean+std, alpha=0.2)\n",
    "    plt.title(f\"{feat} mean\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Value (std)\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_mean_trajectories({\n",
    "  \"original\": orig_xb,\n",
    "  \"pgd\": pgd_perturbed_xb,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1483c3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b041a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monotone_rate(xb):\n",
    "  rates = {}\n",
    "  for feat, seq in mono_groups.items():\n",
    "    if len(seq) < 2: \n",
    "      continue\n",
    "    idxs = [col_idx for (_, col_idx) in seq]\n",
    "    xs = xb[:, idxs].cpu().numpy()  # [N, T]\n",
    "    dirn = feature_direction(feat)\n",
    "    ok = 0\n",
    "    for row in xs:\n",
    "      diffs = np.diff(row)\n",
    "      if dirn == \"inc\":\n",
    "        if np.all(diffs >= -1e-8):  \n",
    "          ok += 1\n",
    "      else:\n",
    "        if np.all(diffs <=  1e-8):\n",
    "          ok += 1\n",
    "    rates[feat] = ok / xs.shape[0]\n",
    "  return rates\n",
    "\n",
    "def total_variation(xb):\n",
    "  tvs = {}\n",
    "  for feat, seq in mono_groups.items():\n",
    "    if len(seq) < 2: continue\n",
    "    idxs = [col_idx for (_, col_idx) in seq]\n",
    "    xs = xb[:, idxs].cpu().numpy()\n",
    "    tvs[feat] = np.mean(np.sum(np.abs(np.diff(xs, axis=1)), axis=1))\n",
    "  return tvs\n",
    "\n",
    "def curvature_penalty(xb):\n",
    "  curvs = {}\n",
    "  for feat, seq in mono_groups.items():\n",
    "    if len(seq) < 3: continue\n",
    "    idxs = [col_idx for (_, col_idx) in seq]\n",
    "    xs = xb[:, idxs].cpu().numpy()\n",
    "    sd = xs[:, 2:] - 2*xs[:, 1:-1] + xs[:, :-2]\n",
    "    curvs[feat] = float(np.mean(np.sum(sd**2, axis=1)))\n",
    "  return curvs\n",
    "\n",
    "\n",
    "orig_xb = torch.cat([xb for xb, _ in val_pre_with_mean_loader])  \n",
    "metrics_df = []\n",
    "for tag, arr in [\n",
    "  (\"original\", orig_xb),\n",
    "  (\"pgd\",      pgd_perturbed_xb),\n",
    "  (\"pgd+iso\",  pgd_iso),\n",
    "]:\n",
    "  mr = monotone_rate(arr)\n",
    "  tv = total_variation(arr)\n",
    "  cv = curvature_penalty(arr)\n",
    "  for feat in mono_groups.keys():\n",
    "    row = {\n",
    "      \"version\": tag,\n",
    "      \"feature\": feat,\n",
    "      \"monotone_rate\": mr.get(feat, np.nan),\n",
    "      \"total_variation\": tv.get(feat, np.nan),\n",
    "      \"curvature\": cv.get(feat, np.nan),\n",
    "    }\n",
    "    metrics_df.append(row)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2b7366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "def isotonic_fix(sequence, direction=\"inc\"):\n",
    "  x = np.arange(len(sequence))\n",
    "  y = sequence\n",
    "  if direction == \"inc\":\n",
    "    ir = IsotonicRegression(increasing=True, out_of_bounds=\"clip\")\n",
    "    return ir.fit_transform(x, y)\n",
    "  else:\n",
    "    ir = IsotonicRegression(increasing=False, out_of_bounds=\"clip\")\n",
    "    return ir.fit_transform(x, y)\n",
    "\n",
    "pgd_iso = pgd_perturbed_xb.clone().cpu().numpy()\n",
    "for feat, seq in mono_groups.items():\n",
    "  idxs = [col_idx for (_, col_idx) in seq]\n",
    "  dirn = feature_direction(feat)\n",
    "  ys = pgd_iso[:, idxs]  # [N, T]\n",
    "  for i in range(ys.shape[0]):\n",
    "    ys[i, :] = isotonic_fix(ys[i, :], direction=dirn)\n",
    "  pgd_iso[:, idxs] = ys\n",
    "\n",
    "pgd_iso = torch.tensor(pgd_iso, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ac3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_feature_mean_trajectories({\n",
    "  \"original\": orig_xb,\n",
    "  \"pgd\": pgd_perturbed_xb,\n",
    "  \"pgd+iso\": pgd_iso,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_feature_mean_trajectories({\n",
    "  \"original\": orig_xb,\n",
    "  \"pgd\": pgd_perturbed_xb,\n",
    "  \"pgd+iso\": pgd_iso,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74504fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
